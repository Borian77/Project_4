{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3389c66e",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb28e90",
   "metadata": {},
   "source": [
    "Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "399f560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user keras-nlp tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ab94cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d46bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-nlp\n",
      "  Using cached keras_nlp-0.9.1-py3-none-any.whl (508 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from keras-nlp) (1.24.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from keras-nlp) (21.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from keras-nlp) (2.1.0)\n",
      "Collecting kagglehub\n",
      "  Using cached kagglehub-0.2.2-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: rich in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from keras-nlp) (13.7.1)\n",
      "Collecting keras-core\n",
      "  Using cached keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from keras-nlp) (2022.7.9)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from keras-nlp) (0.1.8)\n",
      "Collecting tensorflow-text\n",
      "  Using cached tensorflow_text-2.10.0-cp39-cp39-win_amd64.whl (5.0 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from kagglehub->keras-nlp) (4.64.1)\n",
      "Requirement already satisfied: requests in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from kagglehub->keras-nlp) (2.28.1)\n",
      "Requirement already satisfied: namex in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from keras-core->keras-nlp) (0.0.7)\n",
      "Requirement already satisfied: h5py in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from keras-core->keras-nlp) (3.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from packaging->keras-nlp) (3.0.9)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from rich->keras-nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from rich->keras-nlp) (2.17.2)\n",
      "Collecting tensorflow<2.11,>=2.10.0\n",
      "  Using cached tensorflow-2.10.1-cp39-cp39-win_amd64.whl (455.9 MB)\n",
      "Collecting tensorflow-hub>=0.8.0\n",
      "  Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp) (0.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (1.14.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (2.4.0)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Using cached protobuf-3.19.6-cp39-cp39-win_amd64.whl (895 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (0.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (4.11.0)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Using cached tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (24.3.7)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (2.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (18.1.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (63.4.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (1.62.1)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Using cached keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-hub>=0.8.0->tensorflow-text->keras-nlp) (2.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras-nlp) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras-nlp) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras-nlp) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from requests->kagglehub->keras-nlp) (2022.9.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tqdm->kagglehub->keras-nlp) (0.4.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (0.37.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (3.3.4)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (2.0.3)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.11,>=2.10.0->tensorflow-text->keras-nlp) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: keras, tensorboard-data-server, protobuf, oauthlib, keras-preprocessing, gast, cachetools, tensorflow-hub, requests-oauthlib, kagglehub, google-auth, keras-core, google-auth-oauthlib, tensorboard, tensorflow, tensorflow-text, keras-nlp\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.1.1\n",
      "    Uninstalling keras-3.1.1:\n",
      "      Successfully uninstalled keras-3.1.1\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.5.4\n",
      "    Uninstalling gast-0.5.4:\n",
      "      Successfully uninstalled gast-0.5.4\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.16.2\n",
      "    Uninstalling tensorboard-2.16.2:\n",
      "      Successfully uninstalled tensorboard-2.16.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.16.1\n",
      "    Uninstalling tensorflow-2.16.1:\n",
      "      Successfully uninstalled tensorflow-2.16.1\n",
      "  Rolling back uninstall of tensorflow\n",
      "  Moving to c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages\\tensorflow-2.16.1.dist-info\\\n",
      "   from C:\\Users\\aib-amfbc\\anaconda3\\Lib\\site-packages\\~ensorflow-2.16.1.dist-info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Отказано в доступе: 'C:\\\\Users\\\\aib-amfbc\\\\anaconda3\\\\Lib\\\\site-packages\\\\tensorflow\\\\python\\\\platform\\\\_pywrap_tf2.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#!pip install keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7cbda25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.62.1)\n",
      "Collecting keras>=3.0.0\n",
      "  Downloading keras-3.2.0-py3-none-any.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Collecting tensorboard<2.17,>=2.16\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.7)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (21.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.24.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.25.3-cp39-cp39-win_amd64.whl (413 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: namex in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.10.0)\n",
      "Requirement already satisfied: rich in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.0.3)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.16.1->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n",
      "Installing collected packages: tensorboard-data-server, protobuf, tensorboard, keras\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.6.1\n",
      "    Uninstalling tensorboard-data-server-0.6.1:\n",
      "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.10.1\n",
      "    Uninstalling tensorboard-2.10.1:\n",
      "      Successfully uninstalled tensorboard-2.10.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.10.0\n",
      "    Uninstalling keras-2.10.0:\n",
      "      Successfully uninstalled keras-2.10.0\n",
      "Successfully installed keras-3.2.0 protobuf-4.25.3 tensorboard-2.16.2 tensorboard-data-server-0.7.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aib-amfbc\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be405cd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24380\\878077525.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunction_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoordination_config_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mattr_value_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_attr__value__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnode_def_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_node__def__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mop_def_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_op__def__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresource_handle_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_resource__handle__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtypes_pb2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m   \u001b[0mcontaining_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m   fields=[\n\u001b[1;32m---> 36\u001b[1;33m     _descriptor.FieldDescriptor(\n\u001b[0m\u001b[0;32m     37\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tensorflow.TensorShapeProto.Dim.size'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m       \u001b[0mnumber\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcpp_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\protobuf\\descriptor.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 \u001b[0mhas_default_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontaining_oneof\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjson_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m                 file=None, create_key=None):  # pylint: disable=redefined-builtin\n\u001b[1;32m--> 553\u001b[1;33m       \u001b[0m_message\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_CheckCalledFromGeneratedFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mis_extension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_pool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFindExtensionByName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "# Импортируем необходимые библиотеки\n",
    "import os\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "#import spacy\n",
    "#import keras_nlp\n",
    "import sentence_transformers\n",
    "import glob\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import notebook\n",
    "tqdm.pandas()\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from scipy import stats as st\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, GroupShuffleSplit, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn.svm import SVC, LinearSVR\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, log_loss, f1_score, mean_squared_error, mean_absolute_error\n",
    "#from pymystem3 import Mystem\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Conv2D, MaxPooling2D, BatchNormalization, Reshape, RepeatVector, LSTM, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tensorflow import keras\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from statistics import mode\n",
    "\n",
    "nltk.download('stopwords') # поддерживает удаление стоп-слов\n",
    "nltk.download('punkt') # делит текст на список предложений\n",
    "nltk.download('wordnet') # проводит лемматизацию\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c236618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявим глобальные переменные\n",
    "DATA_PATH = 'C:/Users/aib-amfbc/project_4/sources/dsplus_integrated_project_4/to_upload'\n",
    "TRAIN_PATH = 'C:/Users/aib-amfbc/project_4/sources/dsplus_integrated_project_4/to_upload/train_images'\n",
    "TEST_PATH = 'C:/Users/aib-amfbc/project_4/sources/dsplus_integrated_project_4/to_upload/test_images'\n",
    "SEED = 12345\n",
    "BLOCK = ['child', 'boy', 'girl', 'baby', 'teen', 'teenager', 'kid', 'infant', 'youngster', 'kids', 'children', 'boys', 'girls', 'babies', 'teens', 'teenagers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49c43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим данные в датафреймы, в df_crowd и df_expert переименуем столбцы\n",
    "df_train = pd.read_csv(Path(DATA_PATH, 'train_dataset.csv'))\n",
    "df_crowd = pd.read_csv(Path(DATA_PATH, 'CrowdAnnotations.tsv'), sep='\\t', names=['image', 'query_id', 'share_pos', 'count_pos', 'count_neg'])\n",
    "df_expert = pd.read_csv(Path(DATA_PATH, 'ExpertAnnotations.tsv'), sep='\\t', names=['image', 'query_id', 'expert_1', 'expert_2', 'expert_3'])\n",
    "df_queries = pd.read_csv(Path(DATA_PATH, 'test_queries.csv'), index_col=[0], sep='|')\n",
    "df_images = pd.read_csv(Path(DATA_PATH, 'test_images.csv'), sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479adbcc",
   "metadata": {},
   "source": [
    "Загрузка данных прошла успешно, посмотрим на структуру датафреймов и параметры их данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1426603",
   "metadata": {},
   "source": [
    "**train_dataset.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938bfe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d459e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ca0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c345a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5164d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на изображения, входяще в тренировочный датасет.\n",
    "\n",
    "# Создадим загрузчик\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_gen_flow = train_datagen.flow_from_dataframe(\n",
    "    dataframe=df_train,\n",
    "    directory=Path(DATA_PATH, 'train_images'),\n",
    "    x_col='image',\n",
    "    y_col='final_mark',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='input',\n",
    "    seed=SEED,\n",
    "    shuffle=False)\n",
    "\n",
    "features, target = next(train_gen_flow)\n",
    "\n",
    "# Выведем на экран\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i in range(16):\n",
    "    fig.add_subplot(4, 4, i+1)\n",
    "    plt.imshow(features[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205a183c",
   "metadata": {},
   "source": [
    "Количество объектов в датафрейме df_train - 5822;  \n",
    "Количество уникальных изображений в датафрейме df_train - 1000;  \n",
    "Количество уникальных значений в столбце 'query_id- - 977;  \n",
    "Количество уникальных значений в столбце 'query_text- - 977;  \n",
    "Количество пропущенных значений - 0;  \n",
    "Количество явных дубликатов - 0.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6649c12b",
   "metadata": {},
   "source": [
    "**CrowdAnnotations.tsv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff4dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crowd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1cfe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crowd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e370caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crowd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12999454",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crowd.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим графически на доли людей, подтвердивших, что описание соответствует изображению (по балльной оценке 1-4)\n",
    "sns.histplot(df_crowd['share_pos']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf99fd",
   "metadata": {},
   "source": [
    "Количество объектов в датафрейме df_crowd - 47830;    \n",
    "Среднее количество проголосовавших за соответствие в разы ниже, чем тех, кто проголосовал за несоответствие;  \n",
    "Медианное количество оценок для подтвердивших соответствие - 0;  \n",
    "Медианное количество оценок для подтвердивших несоответствие - 3;  \n",
    "На краудсорсинге чаще дают отрицательный ответ (подтверждено графически);  \n",
    "Количество пропущенных значений - 0;  \n",
    "Количество явных дубликатов - 0.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026cfac7",
   "metadata": {},
   "source": [
    "**ExpertAnnotations.tsv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e8aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expert.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067e4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expert.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5ae7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expert.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75695981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим графически на соотношение оценок, данных 3-мя экспертами\n",
    "df_exp_1 = pd.DataFrame(columns = [ 'mark' , 'lable' ])\n",
    "df_exp_2 = pd.DataFrame(columns = [ 'mark' , 'lable' ])\n",
    "df_exp_3 = pd.DataFrame(columns = [ 'mark' , 'lable' ])\n",
    "\n",
    "df_exp_1['mark'] = df_expert['expert_1']\n",
    "df_exp_1['lable'] = 'Эксперт 1'\n",
    "df_exp_2['mark'] = df_expert['expert_2']\n",
    "df_exp_2['lable'] = 'Эксперт 2'\n",
    "df_exp_3['mark'] = df_expert['expert_3']\n",
    "df_exp_3['lable'] = 'Эксперт 3'\n",
    "\n",
    "for_plot = pd.concat([df_exp_1, df_exp_2, df_exp_3])\n",
    "\n",
    "sns.countplot(data = for_plot, x = 'mark', hue = 'lable');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd420bcf",
   "metadata": {},
   "source": [
    "Количество объектов в датафрейме df_expert - 5822;    \n",
    "Среднее значение голоса эксперта 1 - 1.436;  \n",
    "Среднее значение голоса эксперта 2 - 1.624;  \n",
    "Среднее значение голоса эксперта 3 - 1.882;   \n",
    "Медианное значение голоса эксперта 1 - 1;  \n",
    "Медианное значение голоса эксперта 2 - 1;  \n",
    "Медианное значение голоса эксперта 3 - 2;   \n",
    "Количество пропущенных значений - 0;  \n",
    "Количество явных дубликатов - 0.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd49ed9",
   "metadata": {},
   "source": [
    "**test_queries.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae7e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_queries.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c44648",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_queries.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74749b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_queries.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd9bc5",
   "metadata": {},
   "source": [
    "Количество объектов в датафрейме df_queries - 500;  \n",
    "Количество уникальных изображений в датафрейме df_queries - 100;  \n",
    "Количество уникальных значений в столбце 'query_id- - 500;  \n",
    "Количество уникальных значений в столбце 'query_text- - 500;  \n",
    "Количество пропущенных значений - 0;  \n",
    "Количество явных дубликатов - 0.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd316d",
   "metadata": {},
   "source": [
    "**test_images.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbdec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e80a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa45caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d8e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf71e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Познакомимся с изображениями, входящими в тестовый датасет.\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_gen_flow = test_datagen.flow_from_dataframe(\n",
    "    dataframe = df_images,\n",
    "    directory = Path(DATA_PATH, 'test_images'),\n",
    "    x_col ='image',\n",
    "    y_col = None,\n",
    "    target_size = (224, 224),\n",
    "    batch_size = 16,\n",
    "    class_mode = 'input',\n",
    "    seed = SEED,\n",
    "    shaffle=False)\n",
    "\n",
    "features, target = next(test_gen_flow)\n",
    "\n",
    "# Выведем на экран\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i in range(16):\n",
    "    fig.add_subplot(4, 4, i+1)\n",
    "    plt.imshow(features[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af3d67",
   "metadata": {},
   "source": [
    "Количество объектов в датафрейме df_images - 100;  \n",
    "Количество уникальных объектов в датафрейме df_images - 100;  \n",
    "Количество пропущенных значений - 0;  \n",
    "Количество явных дубликатов - 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9791a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на соответствие описаний изображений из обучающего и тестового датасетов.\n",
    "describes = len(set(df_train['query_text']) & set(df_queries['query_text']))\n",
    "describes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f9262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на соответствие названий изображений из обучающего и тестового датасетов.\n",
    "names = len(set(df_train['image']) & set(df_queries['image']))\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409eba78",
   "metadata": {},
   "source": [
    "Промежуточный результат:  \n",
    "Мы загрузили и ознакомились с данными:  \n",
    "- датафрейм df_train:  \n",
    " - Количество объектов - 5822;  \n",
    " - Количество уникальных - 1000;  \n",
    " - Количество уникальных значений+описаний - 977;  \n",
    " - Количество уникальных описаний - 977;  \n",
    "- датафрейм df_crowd:  \n",
    " - Количество объектов - 47830;    \n",
    " - Среднее количество проголосовавших за соответствие в разы ниже, чем тех, кто проголосовал за несоответствие;  \n",
    " - На краудсорсинге чаще дают отрицательный ответ (подтверждено графически);  \n",
    "- датафрейм df_expert:  \n",
    " - Количество объектов - 5822;    \n",
    " - Среднее значение голоса эксперта 1 - 1.436;  \n",
    " - Среднее значение голоса эксперта 2 - 1.624;  \n",
    " - Среднее значение голоса эксперта 3 - 1.882;   \n",
    " - Медианное значение голоса эксперта 1 - 1;  \n",
    " - Медианное значение голоса эксперта 2 - 1;  \n",
    " - Медианное значение голоса эксперта 3 - 2;   \n",
    "- датафрейм df_queries:  \n",
    " - Количество объектов - 500;  \n",
    " - Количество уникальных изображений - 100;  \n",
    " - Количество уникальных значений+описаний - 500;  \n",
    "- датафрейм df_images:  \n",
    " - Количество объектов - 100;  \n",
    " - Количество уникальных объектов - 100;  \n",
    " \n",
    "Количество пропущенных значений во всех датафреймах - 0;  \n",
    "Количество явных дубликатов во всех датафреймах - 0.  \n",
    "Пересечений между наборами данных для обучающего и тестового датафреймов не обнаружено.  \n",
    "В тестовом датафрейме к каждому изображению даны по 5 описаний.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f5110",
   "metadata": {},
   "source": [
    "**Аггрегируем оценки**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0548f51",
   "metadata": {},
   "source": [
    "Нам необходимо прийти к средней оценке экспертов, при этом масштабировать значение в отрезок [0,1]. Так как значения оценок экспертов могут быть 1, 2, 3 и 4, то масштабируем следующим образом:  \n",
    "mark_score = (mark - min)/(max - min), где min = 1, max = 4.  \n",
    "Для посторяющихся значений прменим метод mode().  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4124df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_agg(data):\n",
    "    \n",
    "    if data['expert_1'] != data['expert_2'] != data['expert_3']:\n",
    "        data['expert_score'] = ((data['expert_1'] + data['expert_2'] + data['expert_3'])/3 - 1)/3\n",
    "    else:\n",
    "        data['expert_score'] = (mode([data['expert_1'], data['expert_2'], data['expert_3']]) - 1)/3\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79290d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expert = df_expert.apply(expert_agg, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_expert.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39126d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61fbbdea",
   "metadata": {},
   "source": [
    "Объединим датафреймы с оценками от экспертов (отмасштабированное среднее значение) и с оценками от неэкспертов (значение столбца share_pos). Там, где присутствуют только оценки эксепертов - принимаем ее, там, где толпы - ее, а там, где обе - примем среднюю в соотношении 7:3. При объединении применим метод outer для хотя бы отсутствия потери данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединим датафреймы df_expert и df_crowd\n",
    "df_scores = pd.merge(df_expert, df_crowd, how='outer', on=['image', 'query_id'])\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция агрегации оценок экспертов и толпы\n",
    "def score_agg(data):\n",
    "\n",
    "    if np.isnan(data['expert_score']):\n",
    "        data['score'] = data['share_pos']\n",
    "    elif np.isnan(data['share_pos']):\n",
    "        data['score'] = data['expert_score']\n",
    "    else:\n",
    "        data['score'] = data['expert_score'] * 0.7 + data['share_pos'] * 0.3\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = df_scores.apply(score_agg, axis=1)\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c20e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1549e7a8",
   "metadata": {},
   "source": [
    "Смапим получившиеся оценки с обучающим датасетом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, df_scores[['image', 'query_id', 'score']], how='outer', on=['image', 'query_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4055bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1918b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948261bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определим значения с пропущенной описательной частью для дальнейшего заполнения при помощи данных столбца query_id\n",
    "df_train_notext = df_train[df_train['query_text'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18701a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_df_train(data):\n",
    "    \n",
    "    if pd.isnull(data['query_text']):\n",
    "        query_texts = df_train_notext[df_train_notext['query_id'] == data['query_id']]['query_text']\n",
    "        \n",
    "        if len(query_texts) > 0:\n",
    "            data['query_text'] = query_texts.iloc[0]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccfd089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запишем пропущенные описания в основной тренировочный датасет\n",
    "df_train = df_train.apply(fill_df_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc36b9e",
   "metadata": {},
   "source": [
    "В незначительной части объема датасета не оказалось описательной части - удалим эти данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89380eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943e5d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00767673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e2827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ebb845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим графически на доли оценок\n",
    "sns.histplot(df_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9134a2c",
   "metadata": {},
   "source": [
    "Вывод:  \n",
    "- Итоговую оценку соответствия изображения описанию масштабировали в значения [0, 1]: 0 - не соответсвует, 1 - полностью соответсвует;  \n",
    "- Больше доверия к экспертным оценкам, если экспертных оценок нет, прибегаем к помощи толпы;  \n",
    "- Выбрали метод и аггрегировали оценки;   \n",
    "- Объединили оценки с обучающим набором данных, заполнили пропущенные описания посредством данных столбца query_id;  \n",
    "- По итогу заполнения описаний удалили те записи, которые заполнить не удалось, их было немного.  \n",
    "\n",
    "Датафрейм готов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7da7ca",
   "metadata": {},
   "source": [
    "**2 Проверка данных**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0997d8f5",
   "metadata": {},
   "source": [
    "Создали словарь 'плохих' слов, включив в него обозначение несовершеннолетних лиц. Словарь \"BLOCK\" размещен в объявлении глобальных переменных. Для избавления от нелегального контента будем использовать текстовые описания к изображениям, для этого переведем все описания в нижний регистр, так как все наши слова в \"BLOCK\" записаны именно в нем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86dccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train['query_text'] = df_train['query_text'].str.lower()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61dc36a",
   "metadata": {},
   "source": [
    "Напишем функцию лемматизации, так как форма слов из набора \"BLOCK\" может быть потеряна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим список стоп-слов\n",
    "stop_words = nltk_stopwords.words('english')\n",
    "\n",
    "# Для лемматизации используем WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Определим функцию РОS-тэгирования слов:\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,               #прилагательное\n",
    "                \"N\": wordnet.NOUN,              #существительное\n",
    "                \"V\": wordnet.VERB,              #глагол\n",
    "                \"R\": wordnet.ADV                #наречие\n",
    "               }  \n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# Определим функцию по токенезации и очистке текста\n",
    "def clean_text(text):\n",
    "    # Очистка текста\n",
    "    clean = \" \".join(re.sub(r'[^a-zA-Z]', ' ', text).lower().split())\n",
    "    # Токенизация\n",
    "    word_list = nltk.word_tokenize(clean)\n",
    "    # Очистка от стоп-слов\n",
    "    clean = [w for w in word_list if w not in stop_words]\n",
    "    return clean\n",
    "\n",
    "# Определим функцию по лемматизации слов текста\n",
    "def lemm_text(text):\n",
    "    # Лемматизация слов\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in clean_text(text)])    \n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199bc30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка работы\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "lemm_text(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b965b6f",
   "metadata": {},
   "source": [
    "Напишем функцию для поиска описаний, в которых встречаются слова из набора \"BLOCK\". При этом добавим еще столбец 'block' в датафрейм с описанием изображений для отметки таких изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d949a189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция пометки изображений в соответтсвии со словарем \"BLOCK\".\n",
    "def add_block(data):\n",
    "    \n",
    "    text = lemm_text(data['query_text'])\n",
    "    if [i for i in text if i in BLOCK]:\n",
    "        data['block'] = 1\n",
    "    else:\n",
    "        data['block'] = 0\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda1102",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.apply(add_block, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296e7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8e61e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на соотношение значений 'block'\n",
    "df_train['block'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fd2cbd",
   "metadata": {},
   "source": [
    "Чуть меньше 1/3 данных попали под наше правило блокировки. Выборочно посмотрим на описания, которые попали под блокировнаие и выведем соответствующие изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef90b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выведем описания\n",
    "df_train.query('block==1')['query_text'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeac258",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_image = list(df_train.query('block==1')['image'].sample(15))\n",
    "block_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e838d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "for i in range(15):\n",
    "    fig.add_subplot(5, 5, i+1)\n",
    "    image = Image.open(Path(DATA_PATH, 'train_images', block_image[i]))\n",
    "    plt.suptitle('Примеры блокированных фотографий', fontsize = 15)\n",
    "    plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7cdae8",
   "metadata": {},
   "source": [
    "Исходя из описаний отбор произведен корректно. Исключим из обучающего датасета все блокированные изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a2ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(index=df_train[df_train['block'] == 1].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d1d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb131ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d351701",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3406c77",
   "metadata": {},
   "source": [
    "Теперь удалим ненужный столбец 'block'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2981e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop('block', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510b10b",
   "metadata": {},
   "source": [
    "Очистили обучающий датасет от перечня запрещенных изображений в соответствии с нашим словарем запрещенных слов и описаниями к изображениям."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112530c7",
   "metadata": {},
   "source": [
    "**Векторизация изображений**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e52dd",
   "metadata": {},
   "source": [
    "Выделим главные компоненты изображений с помощью сверточных слоев. Ипользуем уже известную нам архитектуру ResNet-50, посмотрим на слои и исключим полносвязные слои, которые отвечают за конечное предсказание. При этом загрузим модель данной архитектуры, предварительно натренированную на датасете ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e759fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция загрузчика изображений для обучения.\n",
    "\n",
    "def load_train(path = DATA_PATH):\n",
    "\n",
    "    train_datagen = ImageDataGenerator(rescale=1/255.)\n",
    "    train_gen_flow = train_datagen.flow_from_dataframe(\n",
    "        dataframe=df_train,\n",
    "        directory=Path(path, 'train_images'),\n",
    "        x_col='image',\n",
    "        y_col='score',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=16,\n",
    "        class_mode='input',\n",
    "        seed=SEED,\n",
    "        shuffle=False)\n",
    "    \n",
    "    return train_gen_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de122b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция создания модели\n",
    "def create_model(input_shape=(224, 224, 3)):\n",
    "\n",
    "    backbone = ResNet50(input_shape=input_shape,\n",
    "                        weights='imagenet',\n",
    "                        include_top=False)\n",
    "    model = Sequential()\n",
    "    model.add(backbone)\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    optimizer = Adam(learning_rate = 0.001)\n",
    "    model.compile(optimizer = optimizer, loss = 'mean_squared_error', metrics = ['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e568774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция создания векторов (эмбеддингов)\n",
    "def emb_images(model, loaded_train):\n",
    "\n",
    "    predictions = model.predict(loaded_train)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7058fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Функция создания векторов\n",
    "embeds_images = emb_images(create_model(), load_train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae5d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сделаем копию, на всякий случай, не хочется еще час ждать расчета.\n",
    "embeds_images_copy = embeds_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58abefef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_images = embeds_images_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6eae05",
   "metadata": {},
   "source": [
    "Эмбеддинги изображений получены: embeds_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c055babc",
   "metadata": {},
   "source": [
    "**Векторизация текстов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde21037",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Вызовем функцию очистки текста\n",
    "df_train['lemm_query'] = df_train['query_text'].apply(lemm_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe991884",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd0b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стоп-слова\n",
    "stop_words = set(nltk_stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d3da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect_short = TfidfVectorizer(stop_words = stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee064d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix_short = tf_vect_short.fit_transform(df_train[\"lemm_query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix_short.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a666ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b67f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.loc[:, 'image':'lemm_query']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e6743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e685f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf65cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"query_vector\"] = tf_matrix_short.todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e37f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"query_vector\"] = df_train[\"query_vector\"].apply(lambda x: tf.convert_to_tensor([x], dtype=np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f54c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3663fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab88d8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6060a449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa9e5257",
   "metadata": {},
   "source": [
    "Зафиксируем величину векторов описаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3559be",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector_size_short = tf_matrix_short.todense().shape[1]\n",
    "query_vector_size_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cc7136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "660be2e9",
   "metadata": {},
   "source": [
    "C ngram_range=(1, 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89bf9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_vect = TfidfVectorizer(stop_words=stop_words, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_matrix = tf_vect.fit_transform(df_train[\"lemm_query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f375f23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_matrix.todense().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921401e",
   "metadata": {},
   "source": [
    "Увеличим до двусловных n-грамм, опыт использования ембеддингов текстов указывает на положительное влияние на качество сопоставления текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train[\"query_vector\"] = tf_matrix.todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b138edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train[\"query_vector\"] = df_train[\"query_vector\"].apply(lambda x: tf.convert_to_tensor([x], dtype=np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ec605",
   "metadata": {},
   "source": [
    "Зафиксируем величину векторов описаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3028097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_vector_size = tf_matrix.todense().shape[1]\n",
    "#query_vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44d04de",
   "metadata": {},
   "source": [
    "**Объединение векторов**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eb02a6",
   "metadata": {},
   "source": [
    "Подготовьте данные для обучения: объедините векторы изображений и векторы текстов с целевой переменной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix_short.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedings = pd.DataFrame(embeds_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c7505",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedings = pd.DataFrame(tf_matrix_short.todense()) # index=tf_matrix_short.get_feature_names_out(), columns=[\"TF-IDF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff5255",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5544aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate((image_embedings, text_embedings), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db4a75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1015b6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделим таргет\n",
    "target = np.array(df_train['score'])\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3f52cb",
   "metadata": {},
   "source": [
    "Объединили эмбеддинги, получили признаки и выделили целевой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215bb8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "051cc6fa",
   "metadata": {},
   "source": [
    "Вывод по п.2 \"Подготовьте данные к обучению модели\":  \n",
    "\n",
    "- Создали словарь блокирующих слов, включив в него обозначение детей и несовершеннолетних лиц.  \n",
    "- Чуть меньше 1/3 данных попали под наше правило блокировки.  \n",
    "- Исключили из обучающего датасета все блокированные изображения.  \n",
    "- провели Векторизацию изображений.  \n",
    " - Выделили главные компоненты изображений с помощью сверточных слоев. Ипользовали уже известную нам архитектуру ResNet-50, посмотрели на слои и исключим полносвязные слои, которые отвечают за конечное предсказание. При этом загрузили модель данной архитектуры, предварительно натренированную на датасете ImageNet.   \n",
    " - Эмбеддинги изображений получены размерностью 2048.  \n",
    "- провели Векторизацию текстов.  \n",
    "- С помощью TF-IDF были векторизованы описания изображений в тренировочном датасете. Размерность полученных векторов - 954. Применение n-грамм ngram_range = (1, 2) увеличивает размерность вектора в 3.5 раза - до 3596.    \n",
    "- Размерность полученных векторов изображений в 2 раза больше размерности векторов описаний.  \n",
    "- Объединили эмбеддинги, получили признаки и выделили целевой."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7be88",
   "metadata": {},
   "source": [
    "**Обучение модели предсказания соответствия**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2714e0a8",
   "metadata": {},
   "source": [
    "Получим обучающую и тестовую (валидационную) выборки. Сделаем это с помощью GroupShuffleSplit так, чтобы строки с одинаковыми изображениями содержались либо в тестовом, либо в тренировочном датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4785c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, train_size=.8, random_state=SEED)\n",
    "train_indices, test_indices = next(gss.split(X=features, y=target, groups=df_train['image']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49022dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features = features[train_indices], features[test_indices]\n",
    "train_target, test_target = target[train_indices], target[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b0e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3503d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5830747",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48c9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f551a96",
   "metadata": {},
   "source": [
    "Применим масштабирование, так как по условию задачи необходимо использовать линейную модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(train_features)\n",
    "train_features = scaler.transform(train_features)\n",
    "test_features = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fdc018",
   "metadata": {},
   "source": [
    "Выберем метрику, лучшей метрикой, на мой взгляд, будет метрика MAE, можно и RMSE, но за счет возведения в квадрат она будет сильнее реагировать на выбросы предсказания."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfa3742",
   "metadata": {},
   "source": [
    "**LinearRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c653af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LinearRegression(n_jobs=-1)\n",
    "model.fit(train_features, train_target)\n",
    "score = cross_val_score(\n",
    "    model, train_features, train_target, groups=df_train['image'].loc[train_indices],\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=GroupShuffleSplit(random_state=SEED)\n",
    ").mean()\n",
    "abs(score).round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf58040",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_lr = LinearRegression(n_jobs=-1)\n",
    "model_lr.fit(train_features, train_target)\n",
    "predict_lr = model_lr.predict(test_features)\n",
    "print('RMSE:', mean_squared_error(test_target, predict_lr, squared=False))\n",
    "print('MAE:', mean_absolute_error(test_target, predict_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e0b29e",
   "metadata": {},
   "source": [
    "похоже не нравятся данные для LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6e5768",
   "metadata": {},
   "source": [
    "**Ridge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1155f608",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_rd = Ridge(random_state=SEED)\n",
    "model_rd.fit(train_features, train_target)\n",
    "predict_rd = model_rd.predict(test_features)\n",
    "print('RMSE:', mean_squared_error(test_target, predict_rd, squared=False))\n",
    "print('MAE:', mean_absolute_error(test_target, predict_rd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524043d2",
   "metadata": {},
   "source": [
    "**LinearSVR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svr = LinearSVR(random_state=SEED)\n",
    "\n",
    "param_grid = {\n",
    "    'loss':['squared_epsilon_insensitive'],\n",
    "    'C':[1]\n",
    "}\n",
    "\n",
    "gs_svr = GridSearchCV(\n",
    "    model_svr,\n",
    "    param_grid,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    cv=GroupShuffleSplit(n_splits=3, random_state=SEED),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "gs_svr.fit(train_features, train_target, groups=df_train['image'].loc[train_indices])\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a646b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# на тестовых данных (валидационных)\n",
    "pred = gs_svr.predict(X_test)\n",
    "mean_absolute_error(y_test, pred).round(decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0ae2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
